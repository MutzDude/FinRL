{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9q2_QZgdNk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXaoZs2lh1hi"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
        "\n",
        "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
        "\n",
        "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
        "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
        "* **Pytorch Version**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGunVt8oLCVS"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOzAKQ-SLGX6"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sApkDlD9LIZv"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLD2TZSLKZ-"
      },
      "source": [
        "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
        "\n",
        "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
        "\n",
        "\n",
        "* Action: The action space describes the allowed actions that the agent interacts with the\n",
        "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
        "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
        "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
        "values at state s′ and s, respectively\n",
        "\n",
        "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
        "our trading agent observes many different features to better learn in an interactive environment.\n",
        "\n",
        "* Environment: Dow 30 consituents\n",
        "\n",
        "\n",
        "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffsre789LY08"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5_PTmOh1hj"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBHhVysOEzi"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them.\n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lPqeTTwoh1hn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')\n",
        "import datetime\n",
        "\n",
        "%matplotlib inline\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
        "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "\n",
        "from pprint import pprint\n",
        "from finrl.meta.data_processors.processor_alpaca import AlpacaProcessor\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../FinRL-Library\")\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2owTj985RW4"
      },
      "source": [
        "<a id='1.4'></a>\n",
        "## 2.4. Create Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w9A8CN5R5PuZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    DATA_SAVE_DIR,\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        "    TRAIN_START_DATE,\n",
        "    TRAIN_END_DATE,\n",
        "    TEST_START_DATE,\n",
        "    TEST_END_DATE,\n",
        "    TRADE_START_DATE,\n",
        "    TRADE_END_DATE,\n",
        ")\n",
        "\n",
        "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A289rQWMh1hq"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeQ7iS-LoMm"
      },
      "source": [
        "\n",
        "\n",
        "-----\n",
        "class YahooDownloader:\n",
        "    Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "# Part 4: Preprocess Data\n",
        "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
        "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
        "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiFA3pRWPQMO",
        "outputId": "94bbe31e-af8d-4489-d23f-6803a990c3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data cleaning started\n",
            "align start and end dates\n",
            "produce full timestamp index\n",
            "Start processing tickers\n",
            "ticker list complete\n",
            "Start concat and rename\n",
            "Data clean finished!\n",
            "Started adding Indicators\n",
            "Running Loop\n",
            "Restore Timestamps\n",
            "Finished adding Indicators\n",
            "Data cleaning started\n",
            "align start and end dates\n",
            "produce full timestamp index\n",
            "Start processing tickers\n",
            "ticker list complete\n",
            "Start concat and rename\n",
            "Data clean finished!\n"
          ]
        }
      ],
      "source": [
        "TRAIN_START_DATE = '2024-09-01'\n",
        "TRAIN_END_DATE = '2025-01-01'\n",
        "TRADE_START_DATE = '2025-01-01'\n",
        "TRADE_END_DATE = '2025-01-29'\n",
        "\n",
        "train_period=[pd.Timestamp(TRAIN_START_DATE + \" 09:30:00\", tz='America/New_York'),pd.Timestamp(TRAIN_END_DATE + \" 15:59:00\", tz='America/New_York')]\n",
        "val_test_period=[pd.Timestamp(TRADE_START_DATE + \" 09:30:00\", tz='America/New_York'),pd.Timestamp(TRADE_END_DATE + \" 15:59:00\", tz='America/New_York')]\n",
        "\n",
        "\n",
        "symbols = ['NVDA', 'TSLA', 'AAPL', 'META', 'MSFT', 'AVGO', 'AMZN', 'PLTR', 'GOOGL', 'AMD', 'CRM', 'BA', 'GEV', 'NFLX', 'CRWD', 'ORCL', 'GOOG', 'MU', 'VST', 'NOW', 'ETN', 'CEG', 'ADBE', 'LLY', 'ANET', 'RCL', 'COST', 'V', 'JPM', 'LMT', 'XOM', 'UNH', 'UBER', 'PANW', 'GM', 'JNJ', 'TXN', 'BKNG', 'CSCO', 'DELL', 'ACN', 'ABT', 'INTC', 'MA', 'QCOM', 'GS', 'SBUX', 'WMT', 'T']\n",
        "\n",
        "\n",
        "\n",
        "INDICATORS = [\n",
        "    \"macd\",\n",
        "    \"boll_ub\",\n",
        "    \"boll_lb\",\n",
        "    \"rsi_30\",\n",
        "    \"cci_30\",\n",
        "    \"dx_30\",\n",
        "    \"close_30_sma\",\n",
        "    \"close_60_sma\",\n",
        "]\n",
        "\n",
        "alpaca = AlpacaProcessor(API_KEY='PKB2PY7303VGIKFLW3O8', API_SECRET='qa2nftQMSEIlFN4ShWSS2BCDCD5tlR10FIGyJNhw')\n",
        "df = alpaca.download_data(symbols[:20], TRAIN_START_DATE, TRADE_END_DATE, '1Min')\n",
        "processed = alpaca.clean_data(df)\n",
        "processed = alpaca.add_technical_indicator(processed, tech_indicator_list=INDICATORS)\n",
        "processed = alpaca.add_vix(processed)\n",
        "processed = alpaca.add_turbulence(processed)\n",
        "processed = processed.ffill().bfill()\n",
        "processed.sort_values(['timestamp','tic'], axis=0,ignore_index=True,inplace=True)\n",
        "#processed['date'] = processed['timestamp'].dt.strftime(date_format='%Y-%m-%d')\n",
        "processed['date'] = processed['timestamp']\n",
        "processed['vix'] = processed['VIXY']\n",
        "processed.drop(['VIXY','timestamp'], axis=1, inplace=True)\n",
        "processed.reset_index(drop=True,inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5. Design Environment\n",
        "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
        "\n",
        "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2zqII8rMIqn",
        "outputId": "47edb3e0-2d83-4063-b2e8-c1d988e61da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 20, State Space: 201\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(processed.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AWyp84Ltto19"
      },
      "outputs": [],
      "source": [
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"buy_cost_pct\": 0.001,\n",
        "    \"sell_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"print_verbosity\":5\n",
        "\n",
        "}\n",
        "\n",
        "# buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "# num_stock_shares = [0] * stock_dimension\n",
        "# env_kwargs = {\n",
        "#     \"hmax\": 100,\n",
        "#     \"initial_amount\": 1000000,\n",
        "#     \"num_stock_shares\": num_stock_shares,\n",
        "#     \"buy_cost_pct\": buy_cost_list,\n",
        "#     \"sell_cost_pct\": sell_cost_list,\n",
        "#     \"state_space\": state_space,\n",
        "#     \"stock_dim\": stock_dimension,\n",
        "#     \"tech_indicator_list\": INDICATORS,\n",
        "#     \"action_space\": stock_dimension,\n",
        "#     \"reward_scaling\": 1e-4\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "<a id='5'></a>\n",
        "# Part 6: Implement DRL Algorithms\n",
        "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
        "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
        "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
        "design their own DRL algorithms by adapting these DRL algorithms.\n",
        "\n",
        "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v-gthCxMtj1d"
      },
      "outputs": [],
      "source": [
        "rebalance_window = 390 # rebalance_window is the number of days to retrain the model\n",
        "validation_window = 1950 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
        "\n",
        "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
        "                 train_period=train_period,\n",
        "                 val_test_period=val_test_period,\n",
        "                 rebalance_window=rebalance_window,\n",
        "                 validation_window=validation_window,\n",
        "                 **env_kwargs)\n",
        "# e_train_gym = StockTradingEnv(df = processed, **env_kwargs)\n",
        "# agent = DRLAgent(e_train_gym)\n",
        "# if_using_a2c = True\n",
        "# model_a2c = agent.get_model(\"a2c\")\n",
        "# # if if_using_a2c:\n",
        "# #   tmp_path = RESULTS_DIR + '/a2c'\n",
        "# #   new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "# #   model_a2c.set_logger(new_logger_a2c)\n",
        "# trained_a2c = agent.train_model(model=model_a2c,\n",
        "#                              tb_log_name='a2c',\n",
        "#                              total_timesteps=50000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KsfEHa_Etj1d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 60,\n",
        "                    'ent_coef': 0.005,\n",
        "                    'learning_rate': 0.0007\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\":0.01,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.00025,\n",
        "                    \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
        "                      \"buffer_size\": 10_000,\n",
        "                      \"learning_rate\": 0.0005,\n",
        "                      \"batch_size\": 64\n",
        "                    }\n",
        "\n",
        "SAC_model_kwargs = {\n",
        "    \"batch_size\": 64,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "TD3_model_kwargs = {\"batch_size\": 100, \"buffer_size\": 1000000, \"learning_rate\": 0.0001}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "timesteps_dict = {'a2c' : 1_000_000,\n",
        "                 'ppo' : 1_000_000,\n",
        "                 'ddpg' : 1_000_000,\n",
        "                 'sac' : 1_000_000,\n",
        "                 'td3' : 1_000_000\n",
        "                 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1lyCECstj1e",
        "outputId": "056b50cd-f8e8-4192-edd9-f570587ed923",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  452.42677458476606\n",
            "======Model training from:  2024-09-01 09:30:00-04:00 to  2025-01-02 09:30:00-05:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_120_5\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 100          |\n",
            "|    time_elapsed       | 15           |\n",
            "|    total_timesteps    | 500          |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -28.6        |\n",
            "|    explained_variance | 0.45         |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 99           |\n",
            "|    policy_loss        | -5.8         |\n",
            "|    reward             | -0.020388806 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.0422       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 32            |\n",
            "|    iterations         | 200           |\n",
            "|    time_elapsed       | 30            |\n",
            "|    total_timesteps    | 1000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -28.8         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 199           |\n",
            "|    policy_loss        | -0.364        |\n",
            "|    reward             | -0.0026087312 |\n",
            "|    std                | 1.02          |\n",
            "|    value_loss         | 0.000886      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 46          |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -29         |\n",
            "|    explained_variance | -2.05       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 5.93        |\n",
            "|    reward             | 0.012991856 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.0587      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 61           |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -29.2        |\n",
            "|    explained_variance | 5.96e-08     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -3.9         |\n",
            "|    reward             | 0.0044550244 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 0.0282       |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 76          |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -29.4       |\n",
            "|    explained_variance | -6.97       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 0.359       |\n",
            "|    reward             | 0.060691662 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.0466      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 600          |\n",
            "|    time_elapsed       | 92           |\n",
            "|    total_timesteps    | 3000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -29.5        |\n",
            "|    explained_variance | -0.704       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 599          |\n",
            "|    policy_loss        | 1.48         |\n",
            "|    reward             | -0.001335815 |\n",
            "|    std                | 1.06         |\n",
            "|    value_loss         | 0.00638      |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 107        |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -29.7      |\n",
            "|    explained_variance | -5.88      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 0.865      |\n",
            "|    reward             | -0.1275728 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.00134    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 122          |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -29.9        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -1.57        |\n",
            "|    reward             | -0.010115533 |\n",
            "|    std                | 1.08         |\n",
            "|    value_loss         | 0.00377      |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 137       |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -30.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 0.714     |\n",
            "|    reward             | -0.049484 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 0.000768  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 152        |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -30.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -1.49      |\n",
            "|    reward             | 0.04455212 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 0.00274    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 1100         |\n",
            "|    time_elapsed       | 167          |\n",
            "|    total_timesteps    | 5500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -30.5        |\n",
            "|    explained_variance | 1.79e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1099         |\n",
            "|    policy_loss        | 0.154        |\n",
            "|    reward             | -0.050163537 |\n",
            "|    std                | 1.11         |\n",
            "|    value_loss         | 0.000264     |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 182        |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -30.7      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 2.79       |\n",
            "|    reward             | 0.04455871 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 0.011      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 197         |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -31         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -0.516      |\n",
            "|    reward             | 0.030604027 |\n",
            "|    std                | 1.14        |\n",
            "|    value_loss         | 0.000699    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 213         |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -31.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 1.92        |\n",
            "|    reward             | 0.008341774 |\n",
            "|    std                | 1.16        |\n",
            "|    value_loss         | 0.00504     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 228        |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -31.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -0.792     |\n",
            "|    reward             | 0.00195064 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 0.00137    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 243        |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -31.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -6.68      |\n",
            "|    reward             | 0.10282208 |\n",
            "|    std                | 1.18       |\n",
            "|    value_loss         | 0.0506     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 258       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -31.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 0.167     |\n",
            "|    reward             | 0.011106  |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.000408  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 273       |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -32.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -6.33     |\n",
            "|    reward             | 0.0285387 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.0427    |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 1900         |\n",
            "|    time_elapsed       | 288          |\n",
            "|    total_timesteps    | 9500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -32.4        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1899         |\n",
            "|    policy_loss        | -1.21        |\n",
            "|    reward             | -0.045519352 |\n",
            "|    std                | 1.22         |\n",
            "|    value_loss         | 0.00346      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 303         |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -32.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | 1.75        |\n",
            "|    reward             | -0.01112804 |\n",
            "|    std                | 1.24        |\n",
            "|    value_loss         | 0.00296     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 318       |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -32.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 0.3       |\n",
            "|    reward             | 0.0452255 |\n",
            "|    std                | 1.25      |\n",
            "|    value_loss         | 0.0014    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 2200        |\n",
            "|    time_elapsed       | 333         |\n",
            "|    total_timesteps    | 11000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -33.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2199        |\n",
            "|    policy_loss        | -3.92       |\n",
            "|    reward             | -0.02957054 |\n",
            "|    std                | 1.27        |\n",
            "|    value_loss         | 0.0159      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 2300        |\n",
            "|    time_elapsed       | 348         |\n",
            "|    total_timesteps    | 11500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -33.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2299        |\n",
            "|    policy_loss        | 0.0287      |\n",
            "|    reward             | 0.099520884 |\n",
            "|    std                | 1.29        |\n",
            "|    value_loss         | 3.09e-05    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 2400        |\n",
            "|    time_elapsed       | 363         |\n",
            "|    total_timesteps    | 12000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -33.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2399        |\n",
            "|    policy_loss        | 0.252       |\n",
            "|    reward             | 0.013109168 |\n",
            "|    std                | 1.31        |\n",
            "|    value_loss         | 0.00108     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 2500         |\n",
            "|    time_elapsed       | 378          |\n",
            "|    total_timesteps    | 12500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -33.9        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2499         |\n",
            "|    policy_loss        | -5.28        |\n",
            "|    reward             | -0.047642995 |\n",
            "|    std                | 1.32         |\n",
            "|    value_loss         | 0.0337       |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 2600        |\n",
            "|    time_elapsed       | 394         |\n",
            "|    total_timesteps    | 13000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -34.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2599        |\n",
            "|    policy_loss        | -2.29       |\n",
            "|    reward             | -2.6565e-05 |\n",
            "|    std                | 1.34        |\n",
            "|    value_loss         | 0.00559     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 409         |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -34.4       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | 0.266       |\n",
            "|    reward             | 0.066940226 |\n",
            "|    std                | 1.35        |\n",
            "|    value_loss         | 0.000748    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 33          |\n",
            "|    iterations         | 2800        |\n",
            "|    time_elapsed       | 424         |\n",
            "|    total_timesteps    | 14000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -34.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2799        |\n",
            "|    policy_loss        | -0.0311     |\n",
            "|    reward             | -0.07666179 |\n",
            "|    std                | 1.37        |\n",
            "|    value_loss         | 0.000823    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 2900        |\n",
            "|    time_elapsed       | 439         |\n",
            "|    total_timesteps    | 14500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -34.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2899        |\n",
            "|    policy_loss        | -2.98       |\n",
            "|    reward             | -0.13305037 |\n",
            "|    std                | 1.38        |\n",
            "|    value_loss         | 0.00951     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 3000       |\n",
            "|    time_elapsed       | 454        |\n",
            "|    total_timesteps    | 15000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -35        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2999       |\n",
            "|    policy_loss        | -2.24      |\n",
            "|    reward             | 0.05297239 |\n",
            "|    std                | 1.39       |\n",
            "|    value_loss         | 0.00733    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 3100        |\n",
            "|    time_elapsed       | 469         |\n",
            "|    total_timesteps    | 15500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -35.2       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3099        |\n",
            "|    policy_loss        | -2.77       |\n",
            "|    reward             | -0.03927847 |\n",
            "|    std                | 1.41        |\n",
            "|    value_loss         | 0.00736     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 3200        |\n",
            "|    time_elapsed       | 485         |\n",
            "|    total_timesteps    | 16000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -35.5       |\n",
            "|    explained_variance | -0.683      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3199        |\n",
            "|    policy_loss        | 2.06        |\n",
            "|    reward             | -0.21983193 |\n",
            "|    std                | 1.43        |\n",
            "|    value_loss         | 0.0189      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 500      |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -35.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | -4.19    |\n",
            "|    reward             | 0.018991 |\n",
            "|    std                | 1.44     |\n",
            "|    value_loss         | 0.0155   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 3400       |\n",
            "|    time_elapsed       | 515        |\n",
            "|    total_timesteps    | 17000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -35.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3399       |\n",
            "|    policy_loss        | 2.25       |\n",
            "|    reward             | 0.00644078 |\n",
            "|    std                | 1.45       |\n",
            "|    value_loss         | 0.00589    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 530         |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -35.9       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | 0.243       |\n",
            "|    reward             | -0.03374018 |\n",
            "|    std                | 1.46        |\n",
            "|    value_loss         | 0.000342    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 3600        |\n",
            "|    time_elapsed       | 545         |\n",
            "|    total_timesteps    | 18000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -36         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3599        |\n",
            "|    policy_loss        | 2.99        |\n",
            "|    reward             | -0.07777485 |\n",
            "|    std                | 1.47        |\n",
            "|    value_loss         | 0.00908     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 3700         |\n",
            "|    time_elapsed       | 561          |\n",
            "|    total_timesteps    | 18500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -36.1        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3699         |\n",
            "|    policy_loss        | -4.93        |\n",
            "|    reward             | -0.030015696 |\n",
            "|    std                | 1.47         |\n",
            "|    value_loss         | 0.0233       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 3800         |\n",
            "|    time_elapsed       | 576          |\n",
            "|    total_timesteps    | 19000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -36.2        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3799         |\n",
            "|    policy_loss        | 1.89         |\n",
            "|    reward             | -0.008463184 |\n",
            "|    std                | 1.48         |\n",
            "|    value_loss         | 0.00269      |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 3900       |\n",
            "|    time_elapsed       | 591        |\n",
            "|    total_timesteps    | 19500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -36.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3899       |\n",
            "|    policy_loss        | -1.34      |\n",
            "|    reward             | -0.6072964 |\n",
            "|    std                | 1.49       |\n",
            "|    value_loss         | 0.00261    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 4000       |\n",
            "|    time_elapsed       | 606        |\n",
            "|    total_timesteps    | 20000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -36.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3999       |\n",
            "|    policy_loss        | -2.55      |\n",
            "|    reward             | 0.14361952 |\n",
            "|    std                | 1.5        |\n",
            "|    value_loss         | 0.0105     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 4100        |\n",
            "|    time_elapsed       | 622         |\n",
            "|    total_timesteps    | 20500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -36.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4099        |\n",
            "|    policy_loss        | 3.61        |\n",
            "|    reward             | 0.013438568 |\n",
            "|    std                | 1.5         |\n",
            "|    value_loss         | 0.0152      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 637      |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -36.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -2.61    |\n",
            "|    reward             | 0.063704 |\n",
            "|    std                | 1.51     |\n",
            "|    value_loss         | 0.00784  |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 4300       |\n",
            "|    time_elapsed       | 652        |\n",
            "|    total_timesteps    | 21500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -36.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4299       |\n",
            "|    policy_loss        | -3.95      |\n",
            "|    reward             | -0.1725944 |\n",
            "|    std                | 1.52       |\n",
            "|    value_loss         | 0.0155     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 4400        |\n",
            "|    time_elapsed       | 668         |\n",
            "|    total_timesteps    | 22000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -37         |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4399        |\n",
            "|    policy_loss        | -1.61       |\n",
            "|    reward             | 0.074449845 |\n",
            "|    std                | 1.54        |\n",
            "|    value_loss         | 0.00683     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 4500        |\n",
            "|    time_elapsed       | 683         |\n",
            "|    total_timesteps    | 22500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -37.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4499        |\n",
            "|    policy_loss        | 2.58        |\n",
            "|    reward             | -0.09498497 |\n",
            "|    std                | 1.55        |\n",
            "|    value_loss         | 0.00726     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 4600       |\n",
            "|    time_elapsed       | 699        |\n",
            "|    total_timesteps    | 23000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -37.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4599       |\n",
            "|    policy_loss        | 4.69       |\n",
            "|    reward             | 0.12205115 |\n",
            "|    std                | 1.55       |\n",
            "|    value_loss         | 0.0189     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 4700        |\n",
            "|    time_elapsed       | 714         |\n",
            "|    total_timesteps    | 23500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -37.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4699        |\n",
            "|    policy_loss        | -2.53       |\n",
            "|    reward             | -0.04937198 |\n",
            "|    std                | 1.56        |\n",
            "|    value_loss         | 0.00564     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 4800        |\n",
            "|    time_elapsed       | 729         |\n",
            "|    total_timesteps    | 24000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -37.3       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4799        |\n",
            "|    policy_loss        | 6.65        |\n",
            "|    reward             | 0.055249784 |\n",
            "|    std                | 1.57        |\n",
            "|    value_loss         | 0.0428      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 744      |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -37.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -0.0294  |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 1.58     |\n",
            "|    value_loss         | 9.49e-07 |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 5000        |\n",
            "|    time_elapsed       | 760         |\n",
            "|    total_timesteps    | 25000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -37.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4999        |\n",
            "|    policy_loss        | 6.57        |\n",
            "|    reward             | -0.06860707 |\n",
            "|    std                | 1.59        |\n",
            "|    value_loss         | 0.0306      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 5100        |\n",
            "|    time_elapsed       | 775         |\n",
            "|    total_timesteps    | 25500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -37.7       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5099        |\n",
            "|    policy_loss        | 1.58        |\n",
            "|    reward             | 0.012087985 |\n",
            "|    std                | 1.6         |\n",
            "|    value_loss         | 0.00225     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 5200        |\n",
            "|    time_elapsed       | 790         |\n",
            "|    total_timesteps    | 26000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -37.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5199        |\n",
            "|    policy_loss        | -2.1        |\n",
            "|    reward             | -0.02017544 |\n",
            "|    std                | 1.61        |\n",
            "|    value_loss         | 0.00567     |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 805      |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -38      |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -0.278   |\n",
            "|    reward             | 0.019073 |\n",
            "|    std                | 1.62     |\n",
            "|    value_loss         | 0.00177  |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 820        |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -38.2      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | -7.79      |\n",
            "|    reward             | 0.01039699 |\n",
            "|    std                | 1.63       |\n",
            "|    value_loss         | 0.0476     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 5500         |\n",
            "|    time_elapsed       | 835          |\n",
            "|    total_timesteps    | 27500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -38.3        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5499         |\n",
            "|    policy_loss        | -0.429       |\n",
            "|    reward             | -0.019410545 |\n",
            "|    std                | 1.65         |\n",
            "|    value_loss         | 0.00158      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 5600        |\n",
            "|    time_elapsed       | 851         |\n",
            "|    total_timesteps    | 28000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -38.5       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5599        |\n",
            "|    policy_loss        | 1.84        |\n",
            "|    reward             | -0.05846384 |\n",
            "|    std                | 1.66        |\n",
            "|    value_loss         | 0.00389     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 866        |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -38.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | 0.761      |\n",
            "|    reward             | 0.10072107 |\n",
            "|    std                | 1.67       |\n",
            "|    value_loss         | 0.0113     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 5800        |\n",
            "|    time_elapsed       | 881         |\n",
            "|    total_timesteps    | 29000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -38.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5799        |\n",
            "|    policy_loss        | -1.06       |\n",
            "|    reward             | -0.06779906 |\n",
            "|    std                | 1.69        |\n",
            "|    value_loss         | 0.00168     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 5900        |\n",
            "|    time_elapsed       | 896         |\n",
            "|    total_timesteps    | 29500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -39         |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5899        |\n",
            "|    policy_loss        | -0.293      |\n",
            "|    reward             | -1.2716e-05 |\n",
            "|    std                | 1.7         |\n",
            "|    value_loss         | 0.000329    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 911        |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -39.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | 0.169      |\n",
            "|    reward             | 0.11416189 |\n",
            "|    std                | 1.71       |\n",
            "|    value_loss         | 0.0092     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 6100        |\n",
            "|    time_elapsed       | 926         |\n",
            "|    total_timesteps    | 30500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -39.2       |\n",
            "|    explained_variance | -0.328      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6099        |\n",
            "|    policy_loss        | -1.58       |\n",
            "|    reward             | -0.01864554 |\n",
            "|    std                | 1.72        |\n",
            "|    value_loss         | 0.00237     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 942        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -39.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | -1.4       |\n",
            "|    reward             | 0.05340531 |\n",
            "|    std                | 1.73       |\n",
            "|    value_loss         | 0.00127    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 6300       |\n",
            "|    time_elapsed       | 957        |\n",
            "|    total_timesteps    | 31500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -39.6      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6299       |\n",
            "|    policy_loss        | 0.157      |\n",
            "|    reward             | 0.04068177 |\n",
            "|    std                | 1.76       |\n",
            "|    value_loss         | 2.65e-05   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 6400       |\n",
            "|    time_elapsed       | 972        |\n",
            "|    total_timesteps    | 32000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -39.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6399       |\n",
            "|    policy_loss        | -6.59      |\n",
            "|    reward             | 0.14961962 |\n",
            "|    std                | 1.78       |\n",
            "|    value_loss         | 0.0357     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 6500        |\n",
            "|    time_elapsed       | 987         |\n",
            "|    total_timesteps    | 32500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -40         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6499        |\n",
            "|    policy_loss        | -0.21       |\n",
            "|    reward             | -4.6656e-05 |\n",
            "|    std                | 1.79        |\n",
            "|    value_loss         | 0.000472    |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 6600     |\n",
            "|    time_elapsed       | 1003     |\n",
            "|    total_timesteps    | 33000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -40.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | 3.35     |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 1.8      |\n",
            "|    value_loss         | 0.0103   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 6700       |\n",
            "|    time_elapsed       | 1018       |\n",
            "|    total_timesteps    | 33500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -40.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6699       |\n",
            "|    policy_loss        | 0.848      |\n",
            "|    reward             | 0.06148484 |\n",
            "|    std                | 1.82       |\n",
            "|    value_loss         | 0.00106    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 6800        |\n",
            "|    time_elapsed       | 1033        |\n",
            "|    total_timesteps    | 34000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -40.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6799        |\n",
            "|    policy_loss        | -6.1        |\n",
            "|    reward             | -0.24825008 |\n",
            "|    std                | 1.83        |\n",
            "|    value_loss         | 0.0338      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 6900        |\n",
            "|    time_elapsed       | 1049        |\n",
            "|    total_timesteps    | 34500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -40.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6899        |\n",
            "|    policy_loss        | 2.58        |\n",
            "|    reward             | -0.00110133 |\n",
            "|    std                | 1.84        |\n",
            "|    value_loss         | 0.00434     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 1064       |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -40.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 1.39       |\n",
            "|    reward             | -7.629e-06 |\n",
            "|    std                | 1.86       |\n",
            "|    value_loss         | 0.00187    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 1079      |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -40.9     |\n",
            "|    explained_variance | 0.0187    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 19.5      |\n",
            "|    reward             | 0.2590529 |\n",
            "|    std                | 1.87      |\n",
            "|    value_loss         | 0.34      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 1094      |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | 2.23      |\n",
            "|    reward             | 0.0251654 |\n",
            "|    std                | 1.88      |\n",
            "|    value_loss         | 0.00576   |\n",
            "-------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 32            |\n",
            "|    iterations         | 7300          |\n",
            "|    time_elapsed       | 1109          |\n",
            "|    total_timesteps    | 36500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -41.2         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 7299          |\n",
            "|    policy_loss        | 0.0688        |\n",
            "|    reward             | -0.0048576365 |\n",
            "|    std                | 1.9           |\n",
            "|    value_loss         | 0.000867      |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 1124       |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | -2.09      |\n",
            "|    reward             | 0.00642408 |\n",
            "|    std                | 1.92       |\n",
            "|    value_loss         | 0.00352    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 1139       |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | -0.152     |\n",
            "|    reward             | 0.07424166 |\n",
            "|    std                | 1.94       |\n",
            "|    value_loss         | 0.0145     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 7600       |\n",
            "|    time_elapsed       | 1155       |\n",
            "|    total_timesteps    | 38000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7599       |\n",
            "|    policy_loss        | -0.898     |\n",
            "|    reward             | 0.02538493 |\n",
            "|    std                | 1.95       |\n",
            "|    value_loss         | 0.00371    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 7700       |\n",
            "|    time_elapsed       | 1170       |\n",
            "|    total_timesteps    | 38500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7699       |\n",
            "|    policy_loss        | -3.32      |\n",
            "|    reward             | 0.04054206 |\n",
            "|    std                | 1.97       |\n",
            "|    value_loss         | 0.00724    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 7800        |\n",
            "|    time_elapsed       | 1185        |\n",
            "|    total_timesteps    | 39000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7799        |\n",
            "|    policy_loss        | 1.64        |\n",
            "|    reward             | -0.01539666 |\n",
            "|    std                | 1.98        |\n",
            "|    value_loss         | 0.00385     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 1200      |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | -0.911    |\n",
            "|    reward             | 0.1091784 |\n",
            "|    std                | 1.99      |\n",
            "|    value_loss         | 0.00153   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 1215       |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | -1.91      |\n",
            "|    reward             | 0.05260666 |\n",
            "|    std                | 2          |\n",
            "|    value_loss         | 0.00677    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 8100       |\n",
            "|    time_elapsed       | 1230       |\n",
            "|    total_timesteps    | 40500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8099       |\n",
            "|    policy_loss        | 1.8        |\n",
            "|    reward             | -0.0158223 |\n",
            "|    std                | 2.02       |\n",
            "|    value_loss         | 0.00338    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 1245      |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | -4.49     |\n",
            "|    reward             | -0.234104 |\n",
            "|    std                | 2.03      |\n",
            "|    value_loss         | 0.0261    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 8300       |\n",
            "|    time_elapsed       | 1261       |\n",
            "|    total_timesteps    | 41500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8299       |\n",
            "|    policy_loss        | -4.65      |\n",
            "|    reward             | 0.00643317 |\n",
            "|    std                | 2.04       |\n",
            "|    value_loss         | 0.0146     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 8400       |\n",
            "|    time_elapsed       | 1276       |\n",
            "|    total_timesteps    | 42000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8399       |\n",
            "|    policy_loss        | -3.29      |\n",
            "|    reward             | 0.05231769 |\n",
            "|    std                | 2.06       |\n",
            "|    value_loss         | 0.0139     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 1291     |\n",
            "|    total_timesteps    | 42500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | 0.451    |\n",
            "|    reward             | 0.091927 |\n",
            "|    std                | 2.07     |\n",
            "|    value_loss         | 0.00226  |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 8600        |\n",
            "|    time_elapsed       | 1306        |\n",
            "|    total_timesteps    | 43000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8599        |\n",
            "|    policy_loss        | -5.97       |\n",
            "|    reward             | -0.13578524 |\n",
            "|    std                | 2.09        |\n",
            "|    value_loss         | 0.047       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 8700       |\n",
            "|    time_elapsed       | 1321       |\n",
            "|    total_timesteps    | 43500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8699       |\n",
            "|    policy_loss        | 3.93       |\n",
            "|    reward             | 0.03968185 |\n",
            "|    std                | 2.1        |\n",
            "|    value_loss         | 0.0145     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 8800        |\n",
            "|    time_elapsed       | 1336        |\n",
            "|    total_timesteps    | 44000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.3       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8799        |\n",
            "|    policy_loss        | -0.96       |\n",
            "|    reward             | -0.10459763 |\n",
            "|    std                | 2.11        |\n",
            "|    value_loss         | 0.00069     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 1352      |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | -6.84     |\n",
            "|    reward             | 0.0       |\n",
            "|    std                | 2.12      |\n",
            "|    value_loss         | 0.026     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 1367        |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 1.77        |\n",
            "|    reward             | 0.016487803 |\n",
            "|    std                | 2.14        |\n",
            "|    value_loss         | 0.00276     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 1382      |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | -3.41     |\n",
            "|    reward             | -0.021901 |\n",
            "|    std                | 2.16      |\n",
            "|    value_loss         | 0.00835   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 9200       |\n",
            "|    time_elapsed       | 1397       |\n",
            "|    total_timesteps    | 46000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -44        |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9199       |\n",
            "|    policy_loss        | -0.808     |\n",
            "|    reward             | -0.0002439 |\n",
            "|    std                | 2.18       |\n",
            "|    value_loss         | 0.000544   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 9300       |\n",
            "|    time_elapsed       | 1412       |\n",
            "|    total_timesteps    | 46500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -44.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9299       |\n",
            "|    policy_loss        | 4.33       |\n",
            "|    reward             | 0.04303329 |\n",
            "|    std                | 2.21       |\n",
            "|    value_loss         | 0.0121     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 1428      |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -44.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 0.807     |\n",
            "|    reward             | -0.027225 |\n",
            "|    std                | 2.24      |\n",
            "|    value_loss         | 0.00071   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 9500       |\n",
            "|    time_elapsed       | 1443       |\n",
            "|    total_timesteps    | 47500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -44.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9499       |\n",
            "|    policy_loss        | -3.78      |\n",
            "|    reward             | 0.07039832 |\n",
            "|    std                | 2.25       |\n",
            "|    value_loss         | 0.00731    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 9600       |\n",
            "|    time_elapsed       | 1458       |\n",
            "|    total_timesteps    | 48000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -44.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9599       |\n",
            "|    policy_loss        | -0.505     |\n",
            "|    reward             | 0.54344875 |\n",
            "|    std                | 2.26       |\n",
            "|    value_loss         | 0.00247    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 9700        |\n",
            "|    time_elapsed       | 1473        |\n",
            "|    total_timesteps    | 48500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -44.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9699        |\n",
            "|    policy_loss        | -3.63       |\n",
            "|    reward             | -0.02892934 |\n",
            "|    std                | 2.27        |\n",
            "|    value_loss         | 0.0251      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 9800       |\n",
            "|    time_elapsed       | 1488       |\n",
            "|    total_timesteps    | 49000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -44.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9799       |\n",
            "|    policy_loss        | -10.5      |\n",
            "|    reward             | -8.752e-06 |\n",
            "|    std                | 2.28       |\n",
            "|    value_loss         | 0.053      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 9900       |\n",
            "|    time_elapsed       | 1503       |\n",
            "|    total_timesteps    | 49500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9899       |\n",
            "|    policy_loss        | -1.72      |\n",
            "|    reward             | 0.02881007 |\n",
            "|    std                | 2.3        |\n",
            "|    value_loss         | 0.00284    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 1518       |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.1      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | 3.87       |\n",
            "|    reward             | 0.03205342 |\n",
            "|    std                | 2.31       |\n",
            "|    value_loss         | 0.0135     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10100      |\n",
            "|    time_elapsed       | 1534       |\n",
            "|    total_timesteps    | 50500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10099      |\n",
            "|    policy_loss        | -4.06      |\n",
            "|    reward             | 0.00557754 |\n",
            "|    std                | 2.33       |\n",
            "|    value_loss         | 0.00894    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10200      |\n",
            "|    time_elapsed       | 1549       |\n",
            "|    total_timesteps    | 51000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10199      |\n",
            "|    policy_loss        | 0.385      |\n",
            "|    reward             | 0.03491491 |\n",
            "|    std                | 2.34       |\n",
            "|    value_loss         | 0.000998   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 10300       |\n",
            "|    time_elapsed       | 1564        |\n",
            "|    total_timesteps    | 51500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -45.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10299       |\n",
            "|    policy_loss        | -12.6       |\n",
            "|    reward             | -0.15189902 |\n",
            "|    std                | 2.35        |\n",
            "|    value_loss         | 0.0723      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10400      |\n",
            "|    time_elapsed       | 1579       |\n",
            "|    total_timesteps    | 52000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10399      |\n",
            "|    policy_loss        | -43.8      |\n",
            "|    reward             | -0.3815444 |\n",
            "|    std                | 2.36       |\n",
            "|    value_loss         | 1.28       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10500      |\n",
            "|    time_elapsed       | 1595       |\n",
            "|    total_timesteps    | 52500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10499      |\n",
            "|    policy_loss        | -22.2      |\n",
            "|    reward             | 0.30166882 |\n",
            "|    std                | 2.37       |\n",
            "|    value_loss         | 0.265      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10600      |\n",
            "|    time_elapsed       | 1610       |\n",
            "|    total_timesteps    | 53000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10599      |\n",
            "|    policy_loss        | 8.68       |\n",
            "|    reward             | 0.11972669 |\n",
            "|    std                | 2.38       |\n",
            "|    value_loss         | 0.0375     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10700      |\n",
            "|    time_elapsed       | 1625       |\n",
            "|    total_timesteps    | 53500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10699      |\n",
            "|    policy_loss        | -13.6      |\n",
            "|    reward             | 0.13961294 |\n",
            "|    std                | 2.39       |\n",
            "|    value_loss         | 0.0964     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 10800       |\n",
            "|    time_elapsed       | 1640        |\n",
            "|    total_timesteps    | 54000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -45.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10799       |\n",
            "|    policy_loss        | 5.31        |\n",
            "|    reward             | -0.07820109 |\n",
            "|    std                | 2.4         |\n",
            "|    value_loss         | 0.0198      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 10900      |\n",
            "|    time_elapsed       | 1655       |\n",
            "|    total_timesteps    | 54500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -45.9      |\n",
            "|    explained_variance | -0.000175  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10899      |\n",
            "|    policy_loss        | 0.807      |\n",
            "|    reward             | 0.15783662 |\n",
            "|    std                | 2.41       |\n",
            "|    value_loss         | 0.00383    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 11000     |\n",
            "|    time_elapsed       | 1670      |\n",
            "|    total_timesteps    | 55000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -46.1     |\n",
            "|    explained_variance | -0.0229   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10999     |\n",
            "|    policy_loss        | -20       |\n",
            "|    reward             | 0.5053546 |\n",
            "|    std                | 2.42      |\n",
            "|    value_loss         | 0.267     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 11100      |\n",
            "|    time_elapsed       | 1686       |\n",
            "|    total_timesteps    | 55500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -46.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11099      |\n",
            "|    policy_loss        | -2.77      |\n",
            "|    reward             | -0.2231082 |\n",
            "|    std                | 2.43       |\n",
            "|    value_loss         | 0.0139     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 11200       |\n",
            "|    time_elapsed       | 1701        |\n",
            "|    total_timesteps    | 56000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -46.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 11199       |\n",
            "|    policy_loss        | 2.91        |\n",
            "|    reward             | -0.09794747 |\n",
            "|    std                | 2.45        |\n",
            "|    value_loss         | 0.0162      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 11300      |\n",
            "|    time_elapsed       | 1716       |\n",
            "|    total_timesteps    | 56500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -46.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11299      |\n",
            "|    policy_loss        | 3.18       |\n",
            "|    reward             | 0.06995762 |\n",
            "|    std                | 2.46       |\n",
            "|    value_loss         | 0.00574    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 11400      |\n",
            "|    time_elapsed       | 1731       |\n",
            "|    total_timesteps    | 57000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -46.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11399      |\n",
            "|    policy_loss        | 14.4       |\n",
            "|    reward             | 0.08895742 |\n",
            "|    std                | 2.47       |\n",
            "|    value_loss         | 0.113      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 11500    |\n",
            "|    time_elapsed       | 1747     |\n",
            "|    total_timesteps    | 57500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -46.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11499    |\n",
            "|    policy_loss        | 5.34     |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 2.49     |\n",
            "|    value_loss         | 0.0159   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 11600    |\n",
            "|    time_elapsed       | 1762     |\n",
            "|    total_timesteps    | 58000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -46.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11599    |\n",
            "|    policy_loss        | -0.696   |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 2.51     |\n",
            "|    value_loss         | 0.00169  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 11700     |\n",
            "|    time_elapsed       | 1777      |\n",
            "|    total_timesteps    | 58500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -46.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11699     |\n",
            "|    policy_loss        | 11.3      |\n",
            "|    reward             | 1.8391529 |\n",
            "|    std                | 2.52      |\n",
            "|    value_loss         | 0.0671    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 11800    |\n",
            "|    time_elapsed       | 1792     |\n",
            "|    total_timesteps    | 59000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -46.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11799    |\n",
            "|    policy_loss        | 11       |\n",
            "|    reward             | 0.139258 |\n",
            "|    std                | 2.53     |\n",
            "|    value_loss         | 0.0549   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 11900    |\n",
            "|    time_elapsed       | 1808     |\n",
            "|    total_timesteps    | 59500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -47      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11899    |\n",
            "|    policy_loss        | 4.72     |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 2.55     |\n",
            "|    value_loss         | 0.0101   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 12000      |\n",
            "|    time_elapsed       | 1823       |\n",
            "|    total_timesteps    | 60000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -47.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11999      |\n",
            "|    policy_loss        | 5          |\n",
            "|    reward             | 0.03911674 |\n",
            "|    std                | 2.56       |\n",
            "|    value_loss         | 0.0202     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 12100     |\n",
            "|    time_elapsed       | 1838      |\n",
            "|    total_timesteps    | 60500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -47.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12099     |\n",
            "|    policy_loss        | -23.6     |\n",
            "|    reward             | 0.7091753 |\n",
            "|    std                | 2.57      |\n",
            "|    value_loss         | 0.279     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 12200      |\n",
            "|    time_elapsed       | 1853       |\n",
            "|    total_timesteps    | 61000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -47.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 12199      |\n",
            "|    policy_loss        | 3.57       |\n",
            "|    reward             | 0.25020176 |\n",
            "|    std                | 2.58       |\n",
            "|    value_loss         | 0.00898    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 12300       |\n",
            "|    time_elapsed       | 1868        |\n",
            "|    total_timesteps    | 61500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -47.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 12299       |\n",
            "|    policy_loss        | -1.34       |\n",
            "|    reward             | -0.19399638 |\n",
            "|    std                | 2.58        |\n",
            "|    value_loss         | 0.00521     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 12400     |\n",
            "|    time_elapsed       | 1883      |\n",
            "|    total_timesteps    | 62000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -47.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12399     |\n",
            "|    policy_loss        | 3.44      |\n",
            "|    reward             | -0.149738 |\n",
            "|    std                | 2.58      |\n",
            "|    value_loss         | 0.0191    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 12500      |\n",
            "|    time_elapsed       | 1899       |\n",
            "|    total_timesteps    | 62500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -47.3      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 12499      |\n",
            "|    policy_loss        | -25.9      |\n",
            "|    reward             | 0.02637358 |\n",
            "|    std                | 2.59       |\n",
            "|    value_loss         | 0.441      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 12600      |\n",
            "|    time_elapsed       | 1914       |\n",
            "|    total_timesteps    | 63000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -47.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 12599      |\n",
            "|    policy_loss        | 16.9       |\n",
            "|    reward             | 0.05341338 |\n",
            "|    std                | 2.6        |\n",
            "|    value_loss         | 0.13       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 12700       |\n",
            "|    time_elapsed       | 1929        |\n",
            "|    total_timesteps    | 63500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -47.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 12699       |\n",
            "|    policy_loss        | 4.62        |\n",
            "|    reward             | -0.02391952 |\n",
            "|    std                | 2.6         |\n",
            "|    value_loss         | 0.0232      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 12800       |\n",
            "|    time_elapsed       | 1945        |\n",
            "|    total_timesteps    | 64000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -47.6       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 12799       |\n",
            "|    policy_loss        | 2.77        |\n",
            "|    reward             | -0.10752169 |\n",
            "|    std                | 2.62        |\n",
            "|    value_loss         | 0.00726     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 12900     |\n",
            "|    time_elapsed       | 1960      |\n",
            "|    total_timesteps    | 64500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -47.6     |\n",
            "|    explained_variance | -0.000203 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12899     |\n",
            "|    policy_loss        | -2.54     |\n",
            "|    reward             | 0.0       |\n",
            "|    std                | 2.63      |\n",
            "|    value_loss         | 0.0187    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 13000      |\n",
            "|    time_elapsed       | 1975       |\n",
            "|    total_timesteps    | 65000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -47.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 12999      |\n",
            "|    policy_loss        | 5.18       |\n",
            "|    reward             | -7.879e-06 |\n",
            "|    std                | 2.64       |\n",
            "|    value_loss         | 0.0118     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 13100      |\n",
            "|    time_elapsed       | 1990       |\n",
            "|    total_timesteps    | 65500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -47.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13099      |\n",
            "|    policy_loss        | 7.89       |\n",
            "|    reward             | -0.0378948 |\n",
            "|    std                | 2.65       |\n",
            "|    value_loss         | 0.0482     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 13200       |\n",
            "|    time_elapsed       | 2006        |\n",
            "|    total_timesteps    | 66000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -48         |\n",
            "|    explained_variance | 2.98e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 13199       |\n",
            "|    policy_loss        | 9.53        |\n",
            "|    reward             | -0.02480207 |\n",
            "|    std                | 2.67        |\n",
            "|    value_loss         | 0.0421      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 13300     |\n",
            "|    time_elapsed       | 2021      |\n",
            "|    total_timesteps    | 66500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -48.1     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13299     |\n",
            "|    policy_loss        | 1.24      |\n",
            "|    reward             | 0.0       |\n",
            "|    std                | 2.68      |\n",
            "|    value_loss         | 0.00152   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 13400      |\n",
            "|    time_elapsed       | 2036       |\n",
            "|    total_timesteps    | 67000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -48.2      |\n",
            "|    explained_variance | 0.103      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13399      |\n",
            "|    policy_loss        | -0.416     |\n",
            "|    reward             | 0.00241735 |\n",
            "|    std                | 2.7        |\n",
            "|    value_loss         | 0.00133    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 13500      |\n",
            "|    time_elapsed       | 2052       |\n",
            "|    total_timesteps    | 67500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -48.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13499      |\n",
            "|    policy_loss        | 11.1       |\n",
            "|    reward             | 0.21078205 |\n",
            "|    std                | 2.73       |\n",
            "|    value_loss         | 0.0661     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 13600    |\n",
            "|    time_elapsed       | 2067     |\n",
            "|    total_timesteps    | 68000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -48.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13599    |\n",
            "|    policy_loss        | 12.4     |\n",
            "|    reward             | 0.00594  |\n",
            "|    std                | 2.74     |\n",
            "|    value_loss         | 0.071    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 13700     |\n",
            "|    time_elapsed       | 2082      |\n",
            "|    total_timesteps    | 68500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -48.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13699     |\n",
            "|    policy_loss        | -1.92     |\n",
            "|    reward             | -0.006734 |\n",
            "|    std                | 2.77      |\n",
            "|    value_loss         | 0.00257   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 13800      |\n",
            "|    time_elapsed       | 2097       |\n",
            "|    total_timesteps    | 69000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -48.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13799      |\n",
            "|    policy_loss        | 1.19       |\n",
            "|    reward             | 0.00170168 |\n",
            "|    std                | 2.79       |\n",
            "|    value_loss         | 0.000803   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 13900    |\n",
            "|    time_elapsed       | 2113     |\n",
            "|    total_timesteps    | 69500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -49      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13899    |\n",
            "|    policy_loss        | 6.76     |\n",
            "|    reward             | 0.085155 |\n",
            "|    std                | 2.81     |\n",
            "|    value_loss         | 0.0222   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 14000      |\n",
            "|    time_elapsed       | 2128       |\n",
            "|    total_timesteps    | 70000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -49.2      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13999      |\n",
            "|    policy_loss        | -1.96      |\n",
            "|    reward             | -0.0340215 |\n",
            "|    std                | 2.84       |\n",
            "|    value_loss         | 0.00223    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 14100     |\n",
            "|    time_elapsed       | 2143      |\n",
            "|    total_timesteps    | 70500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -49.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 14099     |\n",
            "|    policy_loss        | 2.77      |\n",
            "|    reward             | -0.001412 |\n",
            "|    std                | 2.85      |\n",
            "|    value_loss         | 0.00505   |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 14200        |\n",
            "|    time_elapsed       | 2159         |\n",
            "|    total_timesteps    | 71000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -49.4        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 14199        |\n",
            "|    policy_loss        | -29.8        |\n",
            "|    reward             | -0.048852976 |\n",
            "|    std                | 2.87         |\n",
            "|    value_loss         | 0.473        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 14300        |\n",
            "|    time_elapsed       | 2174         |\n",
            "|    total_timesteps    | 71500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -49.6        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 14299        |\n",
            "|    policy_loss        | -2.26        |\n",
            "|    reward             | -0.008028156 |\n",
            "|    std                | 2.89         |\n",
            "|    value_loss         | 0.0194       |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 14400       |\n",
            "|    time_elapsed       | 2189        |\n",
            "|    total_timesteps    | 72000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -49.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 14399       |\n",
            "|    policy_loss        | 1.23        |\n",
            "|    reward             | 0.017627055 |\n",
            "|    std                | 2.9         |\n",
            "|    value_loss         | 0.0107      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 14500    |\n",
            "|    time_elapsed       | 2204     |\n",
            "|    total_timesteps    | 72500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -49.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14499    |\n",
            "|    policy_loss        | -6.68    |\n",
            "|    reward             | 0.029988 |\n",
            "|    std                | 2.92     |\n",
            "|    value_loss         | 0.0224   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 14600      |\n",
            "|    time_elapsed       | 2219       |\n",
            "|    total_timesteps    | 73000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -49.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 14599      |\n",
            "|    policy_loss        | -1.05      |\n",
            "|    reward             | 0.01476856 |\n",
            "|    std                | 2.93       |\n",
            "|    value_loss         | 0.00578    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 14700       |\n",
            "|    time_elapsed       | 2234        |\n",
            "|    total_timesteps    | 73500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -49.9       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 14699       |\n",
            "|    policy_loss        | -14.2       |\n",
            "|    reward             | -0.05563442 |\n",
            "|    std                | 2.94        |\n",
            "|    value_loss         | 0.0783      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 14800        |\n",
            "|    time_elapsed       | 2249         |\n",
            "|    total_timesteps    | 74000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -50.1        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 14799        |\n",
            "|    policy_loss        | -5.9         |\n",
            "|    reward             | -0.059802275 |\n",
            "|    std                | 2.96         |\n",
            "|    value_loss         | 0.0235       |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 14900      |\n",
            "|    time_elapsed       | 2265       |\n",
            "|    total_timesteps    | 74500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -50.1      |\n",
            "|    explained_variance | -0.0402    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 14899      |\n",
            "|    policy_loss        | 46.5       |\n",
            "|    reward             | 0.11437411 |\n",
            "|    std                | 2.97       |\n",
            "|    value_loss         | 1.11       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 15000        |\n",
            "|    time_elapsed       | 2280         |\n",
            "|    total_timesteps    | 75000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -50.2        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 14999        |\n",
            "|    policy_loss        | 8.45         |\n",
            "|    reward             | -0.073906966 |\n",
            "|    std                | 2.99         |\n",
            "|    value_loss         | 0.0481       |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 15100      |\n",
            "|    time_elapsed       | 2295       |\n",
            "|    total_timesteps    | 75500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -50.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 15099      |\n",
            "|    policy_loss        | 0.0755     |\n",
            "|    reward             | 0.06417809 |\n",
            "|    std                | 2.99       |\n",
            "|    value_loss         | 0.00387    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 15200      |\n",
            "|    time_elapsed       | 2310       |\n",
            "|    total_timesteps    | 76000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -50.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 15199      |\n",
            "|    policy_loss        | 2.4        |\n",
            "|    reward             | 0.06451081 |\n",
            "|    std                | 3.01       |\n",
            "|    value_loss         | 0.00552    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 15300       |\n",
            "|    time_elapsed       | 2325        |\n",
            "|    total_timesteps    | 76500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -50.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 15299       |\n",
            "|    policy_loss        | -3.83       |\n",
            "|    reward             | -0.05042916 |\n",
            "|    std                | 3.02        |\n",
            "|    value_loss         | 0.0164      |\n",
            "---------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 32             |\n",
            "|    iterations         | 15400          |\n",
            "|    time_elapsed       | 2341           |\n",
            "|    total_timesteps    | 77000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -50.6          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 15399          |\n",
            "|    policy_loss        | 6.09           |\n",
            "|    reward             | -0.00050174317 |\n",
            "|    std                | 3.05           |\n",
            "|    value_loss         | 0.0136         |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 15500       |\n",
            "|    time_elapsed       | 2356        |\n",
            "|    total_timesteps    | 77500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -50.8       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 15499       |\n",
            "|    policy_loss        | 1.94        |\n",
            "|    reward             | 0.020597126 |\n",
            "|    std                | 3.07        |\n",
            "|    value_loss         | 0.00374     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 15600     |\n",
            "|    time_elapsed       | 2371      |\n",
            "|    total_timesteps    | 78000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -50.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15599     |\n",
            "|    policy_loss        | -5.48     |\n",
            "|    reward             | 1.9472768 |\n",
            "|    std                | 3.09      |\n",
            "|    value_loss         | 0.018     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 15700       |\n",
            "|    time_elapsed       | 2387        |\n",
            "|    total_timesteps    | 78500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -51         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 15699       |\n",
            "|    policy_loss        | 9.83        |\n",
            "|    reward             | 0.013472882 |\n",
            "|    std                | 3.1         |\n",
            "|    value_loss         | 0.0376      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 32            |\n",
            "|    iterations         | 15800         |\n",
            "|    time_elapsed       | 2402          |\n",
            "|    total_timesteps    | 79000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -51.1         |\n",
            "|    explained_variance | 1.19e-07      |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 15799         |\n",
            "|    policy_loss        | -1.17         |\n",
            "|    reward             | -0.0011883818 |\n",
            "|    std                | 3.12          |\n",
            "|    value_loss         | 0.00168       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 15900        |\n",
            "|    time_elapsed       | 2417         |\n",
            "|    total_timesteps    | 79500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -51.2        |\n",
            "|    explained_variance | 0.00959      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 15899        |\n",
            "|    policy_loss        | -5.67        |\n",
            "|    reward             | -0.050018594 |\n",
            "|    std                | 3.14         |\n",
            "|    value_loss         | 0.0143       |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 16000      |\n",
            "|    time_elapsed       | 2432       |\n",
            "|    total_timesteps    | 80000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -51.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 15999      |\n",
            "|    policy_loss        | -6.11      |\n",
            "|    reward             | -0.0594088 |\n",
            "|    std                | 3.16       |\n",
            "|    value_loss         | 0.0411     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 16100      |\n",
            "|    time_elapsed       | 2447       |\n",
            "|    total_timesteps    | 80500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -51.5      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 16099      |\n",
            "|    policy_loss        | 0.535      |\n",
            "|    reward             | 0.20827578 |\n",
            "|    std                | 3.18       |\n",
            "|    value_loss         | 0.00118    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 32           |\n",
            "|    iterations         | 16200        |\n",
            "|    time_elapsed       | 2462         |\n",
            "|    total_timesteps    | 81000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -51.6        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 16199        |\n",
            "|    policy_loss        | -17.6        |\n",
            "|    reward             | -0.054652438 |\n",
            "|    std                | 3.21         |\n",
            "|    value_loss         | 0.149        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 16300      |\n",
            "|    time_elapsed       | 2478       |\n",
            "|    total_timesteps    | 81500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -51.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 16299      |\n",
            "|    policy_loss        | 1.4        |\n",
            "|    reward             | -0.1009755 |\n",
            "|    std                | 3.22       |\n",
            "|    value_loss         | 0.00233    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 16400      |\n",
            "|    time_elapsed       | 2493       |\n",
            "|    total_timesteps    | 82000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -51.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 16399      |\n",
            "|    policy_loss        | -19.6      |\n",
            "|    reward             | -0.1725905 |\n",
            "|    std                | 3.23       |\n",
            "|    value_loss         | 0.214      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 16500       |\n",
            "|    time_elapsed       | 2508        |\n",
            "|    total_timesteps    | 82500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -51.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 16499       |\n",
            "|    policy_loss        | 18.9        |\n",
            "|    reward             | -0.05753821 |\n",
            "|    std                | 3.25        |\n",
            "|    value_loss         | 0.142       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 16600       |\n",
            "|    time_elapsed       | 2524        |\n",
            "|    total_timesteps    | 83000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -52         |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 16599       |\n",
            "|    policy_loss        | -3.41       |\n",
            "|    reward             | -0.06880996 |\n",
            "|    std                | 3.27        |\n",
            "|    value_loss         | 0.0055      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 16700      |\n",
            "|    time_elapsed       | 2539       |\n",
            "|    total_timesteps    | 83500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -52.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 16699      |\n",
            "|    policy_loss        | -6.07      |\n",
            "|    reward             | 0.11502555 |\n",
            "|    std                | 3.29       |\n",
            "|    value_loss         | 0.0798     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 16800    |\n",
            "|    time_elapsed       | 2554     |\n",
            "|    total_timesteps    | 84000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -52.2    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16799    |\n",
            "|    policy_loss        | -2.88    |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 3.3      |\n",
            "|    value_loss         | 0.00768  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 16900     |\n",
            "|    time_elapsed       | 2569      |\n",
            "|    total_timesteps    | 84500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -52.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 16899     |\n",
            "|    policy_loss        | 14.4      |\n",
            "|    reward             | -0.042822 |\n",
            "|    std                | 3.31      |\n",
            "|    value_loss         | 0.135     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 17000       |\n",
            "|    time_elapsed       | 2584        |\n",
            "|    total_timesteps    | 85000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -52.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 16999       |\n",
            "|    policy_loss        | -13         |\n",
            "|    reward             | -0.07496208 |\n",
            "|    std                | 3.33        |\n",
            "|    value_loss         | 0.0682      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 17100      |\n",
            "|    time_elapsed       | 2599       |\n",
            "|    total_timesteps    | 85500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -52.4      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 17099      |\n",
            "|    policy_loss        | 12.6       |\n",
            "|    reward             | 0.24028075 |\n",
            "|    std                | 3.34       |\n",
            "|    value_loss         | 0.0621     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 17200      |\n",
            "|    time_elapsed       | 2615       |\n",
            "|    total_timesteps    | 86000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -52.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 17199      |\n",
            "|    policy_loss        | -13        |\n",
            "|    reward             | 0.15766326 |\n",
            "|    std                | 3.34       |\n",
            "|    value_loss         | 0.0621     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 17300       |\n",
            "|    time_elapsed       | 2630        |\n",
            "|    total_timesteps    | 86500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -52.5       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 17299       |\n",
            "|    policy_loss        | 6.29        |\n",
            "|    reward             | -0.03519864 |\n",
            "|    std                | 3.35        |\n",
            "|    value_loss         | 0.024       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 17400      |\n",
            "|    time_elapsed       | 2645       |\n",
            "|    total_timesteps    | 87000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -52.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 17399      |\n",
            "|    policy_loss        | 0.554      |\n",
            "|    reward             | -0.3271039 |\n",
            "|    std                | 3.36       |\n",
            "|    value_loss         | 0.0877     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 17500     |\n",
            "|    time_elapsed       | 2660      |\n",
            "|    total_timesteps    | 87500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -52.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 17499     |\n",
            "|    policy_loss        | 12.8      |\n",
            "|    reward             | 0.0570025 |\n",
            "|    std                | 3.37      |\n",
            "|    value_loss         | 0.0986    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 17600      |\n",
            "|    time_elapsed       | 2675       |\n",
            "|    total_timesteps    | 88000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -52.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 17599      |\n",
            "|    policy_loss        | -7.64      |\n",
            "|    reward             | -0.0990465 |\n",
            "|    std                | 3.38       |\n",
            "|    value_loss         | 0.0394     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 17700       |\n",
            "|    time_elapsed       | 2690        |\n",
            "|    total_timesteps    | 88500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -52.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 17699       |\n",
            "|    policy_loss        | 7.66        |\n",
            "|    reward             | 0.047133602 |\n",
            "|    std                | 3.39        |\n",
            "|    value_loss         | 0.0234      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 17800    |\n",
            "|    time_elapsed       | 2705     |\n",
            "|    total_timesteps    | 89000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -52.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17799    |\n",
            "|    policy_loss        | -3.31    |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 3.4      |\n",
            "|    value_loss         | 0.144    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 17900    |\n",
            "|    time_elapsed       | 2720     |\n",
            "|    total_timesteps    | 89500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -52.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17899    |\n",
            "|    policy_loss        | 6.04     |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 3.4      |\n",
            "|    value_loss         | 0.0189   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 18000    |\n",
            "|    time_elapsed       | 2736     |\n",
            "|    total_timesteps    | 90000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -52.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17999    |\n",
            "|    policy_loss        | 0.0223   |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 3.42     |\n",
            "|    value_loss         | 2.21e-07 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 18100     |\n",
            "|    time_elapsed       | 2751      |\n",
            "|    total_timesteps    | 90500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -53       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 18099     |\n",
            "|    policy_loss        | -22.3     |\n",
            "|    reward             | 1.0116165 |\n",
            "|    std                | 3.43      |\n",
            "|    value_loss         | 0.609     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 18200       |\n",
            "|    time_elapsed       | 2766        |\n",
            "|    total_timesteps    | 91000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -53.1       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 18199       |\n",
            "|    policy_loss        | -5.31       |\n",
            "|    reward             | -0.12230085 |\n",
            "|    std                | 3.44        |\n",
            "|    value_loss         | 0.0365      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 18300    |\n",
            "|    time_elapsed       | 2781     |\n",
            "|    total_timesteps    | 91500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -53.1    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18299    |\n",
            "|    policy_loss        | -10      |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 3.45     |\n",
            "|    value_loss         | 0.0566   |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 18400       |\n",
            "|    time_elapsed       | 2797        |\n",
            "|    total_timesteps    | 92000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -53.2       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 18399       |\n",
            "|    policy_loss        | 12.2        |\n",
            "|    reward             | -0.10353545 |\n",
            "|    std                | 3.46        |\n",
            "|    value_loss         | 0.0636      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 18500     |\n",
            "|    time_elapsed       | 2812      |\n",
            "|    total_timesteps    | 92500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -53.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 18499     |\n",
            "|    policy_loss        | 4.72      |\n",
            "|    reward             | -0.310563 |\n",
            "|    std                | 3.47      |\n",
            "|    value_loss         | 0.0133    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 18600       |\n",
            "|    time_elapsed       | 2827        |\n",
            "|    total_timesteps    | 93000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -53.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 18599       |\n",
            "|    policy_loss        | -7.77       |\n",
            "|    reward             | -0.02867516 |\n",
            "|    std                | 3.48        |\n",
            "|    value_loss         | 0.0242      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 18700    |\n",
            "|    time_elapsed       | 2843     |\n",
            "|    total_timesteps    | 93500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -53.4    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18699    |\n",
            "|    policy_loss        | -4.32    |\n",
            "|    reward             | 0.174011 |\n",
            "|    std                | 3.49     |\n",
            "|    value_loss         | 0.0146   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 18800      |\n",
            "|    time_elapsed       | 2858       |\n",
            "|    total_timesteps    | 94000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -53.4      |\n",
            "|    explained_variance | 0.000356   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 18799      |\n",
            "|    policy_loss        | 74.5       |\n",
            "|    reward             | 0.17197424 |\n",
            "|    std                | 3.51       |\n",
            "|    value_loss         | 2.14       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 18900      |\n",
            "|    time_elapsed       | 2874       |\n",
            "|    total_timesteps    | 94500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -53.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 18899      |\n",
            "|    policy_loss        | 17.5       |\n",
            "|    reward             | 0.13124336 |\n",
            "|    std                | 3.53       |\n",
            "|    value_loss         | 0.137      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 19000      |\n",
            "|    time_elapsed       | 2889       |\n",
            "|    total_timesteps    | 95000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -53.6      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 18999      |\n",
            "|    policy_loss        | -4.19      |\n",
            "|    reward             | -0.1026335 |\n",
            "|    std                | 3.54       |\n",
            "|    value_loss         | 0.0117     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 19100       |\n",
            "|    time_elapsed       | 2904        |\n",
            "|    total_timesteps    | 95500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -53.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 19099       |\n",
            "|    policy_loss        | 17.4        |\n",
            "|    reward             | -0.16398439 |\n",
            "|    std                | 3.55        |\n",
            "|    value_loss         | 0.128       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 19200      |\n",
            "|    time_elapsed       | 2920       |\n",
            "|    total_timesteps    | 96000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -53.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 19199      |\n",
            "|    policy_loss        | -11.4      |\n",
            "|    reward             | 0.08159574 |\n",
            "|    std                | 3.55       |\n",
            "|    value_loss         | 0.0883     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 32         |\n",
            "|    iterations         | 19300      |\n",
            "|    time_elapsed       | 2935       |\n",
            "|    total_timesteps    | 96500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -53.7      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 19299      |\n",
            "|    policy_loss        | 6.77       |\n",
            "|    reward             | 0.13091739 |\n",
            "|    std                | 3.56       |\n",
            "|    value_loss         | 0.021      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 19400    |\n",
            "|    time_elapsed       | 2950     |\n",
            "|    total_timesteps    | 97000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -53.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19399    |\n",
            "|    policy_loss        | -0.345   |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 3.58     |\n",
            "|    value_loss         | 5.06e-05 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 19500     |\n",
            "|    time_elapsed       | 2965      |\n",
            "|    total_timesteps    | 97500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -53.9     |\n",
            "|    explained_variance | -0.0231   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 19499     |\n",
            "|    policy_loss        | -13       |\n",
            "|    reward             | -3.490616 |\n",
            "|    std                | 3.6       |\n",
            "|    value_loss         | 0.0692    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 32        |\n",
            "|    iterations         | 19600     |\n",
            "|    time_elapsed       | 2980      |\n",
            "|    total_timesteps    | 98000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -54       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 19599     |\n",
            "|    policy_loss        | -1.1      |\n",
            "|    reward             | -0.102949 |\n",
            "|    std                | 3.61      |\n",
            "|    value_loss         | 0.000948  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 19700    |\n",
            "|    time_elapsed       | 2996     |\n",
            "|    total_timesteps    | 98500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -54      |\n",
            "|    explained_variance | 0.0103   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19699    |\n",
            "|    policy_loss        | -1.32    |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 3.62     |\n",
            "|    value_loss         | 0.00335  |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 19800       |\n",
            "|    time_elapsed       | 3011        |\n",
            "|    total_timesteps    | 99000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -54.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 19799       |\n",
            "|    policy_loss        | 8.52        |\n",
            "|    reward             | -0.04986852 |\n",
            "|    std                | 3.64        |\n",
            "|    value_loss         | 0.0266      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 32       |\n",
            "|    iterations         | 19900    |\n",
            "|    time_elapsed       | 3026     |\n",
            "|    total_timesteps    | 99500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -54.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19899    |\n",
            "|    policy_loss        | 3.87     |\n",
            "|    reward             | 0.028125 |\n",
            "|    std                | 3.66     |\n",
            "|    value_loss         | 0.0186   |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 32          |\n",
            "|    iterations         | 20000       |\n",
            "|    time_elapsed       | 3041        |\n",
            "|    total_timesteps    | 100000      |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -54.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 19999       |\n",
            "|    policy_loss        | 0.0623      |\n",
            "|    reward             | 0.026008124 |\n",
            "|    std                | 3.68        |\n",
            "|    value_loss         | 0.00129     |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2025-01-02 09:30:00-05:00 to  2025-01-02 10:30:00-05:00\n",
            "a2c Sharpe Ratio:  -0.11981947561005393\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_120_1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_summary \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_ensemble_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA2C_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mPPO_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mDDPG_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mSAC_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mTD3_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mtimesteps_dict\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/trading/modules/FinRL/finrl/agents/stablebaselines3/models.py:577\u001b[0m, in \u001b[0;36mDRLEnsembleAgent.run_ensemble_strategy\u001b[0;34m(self, A2C_model_kwargs, PPO_model_kwargs, DDPG_model_kwargs, SAC_model_kwargs, TD3_model_kwargs, timesteps_dict)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# print(\"training: \",len(data_split(df, start=20090000, end=test.datadate.unique()[i-rebalance_window]) ))\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# print(\"==============Model Training===========\")\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Train Each Model\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m MODELS\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Train The Model\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m     model, sharpe_list, sharpe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_window\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharpe_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_start_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_end_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mturbulence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# Save the model's sharpe ratios, and the model itself\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     model_dct[model_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msharpe_list\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sharpe_list\n",
            "File \u001b[0;32m~/trading/modules/FinRL/finrl/agents/stablebaselines3/models.py:374\u001b[0m, in \u001b[0;36mDRLEnsembleAgent._train_window\u001b[0;34m(self, model_name, model_kwargs, sharpe_list, validation_start_date, validation_end_date, timesteps_dict, i, validation, turbulence_threshold)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    371\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model(\n\u001b[1;32m    372\u001b[0m     model_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env, policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs\n\u001b[1;32m    373\u001b[0m )\n\u001b[0;32m--> 374\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43miter_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 100_000\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Validation from: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    383\u001b[0m     validation_start_date,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    385\u001b[0m     validation_end_date,\n\u001b[1;32m    386\u001b[0m )\n\u001b[1;32m    387\u001b[0m val_env \u001b[38;5;241m=\u001b[39m DummyVecEnv(\n\u001b[1;32m    388\u001b[0m     [\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: StockTradingEnv(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m     ]\n\u001b[1;32m    408\u001b[0m )\n",
            "File \u001b[0;32m~/trading/modules/FinRL/finrl/agents/stablebaselines3/models.py:228\u001b[0m, in \u001b[0;36mDRLEnsembleAgent.train_model\u001b[0;34m(model, model_name, tb_log_name, iter_num, total_timesteps)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(model, model_name, tb_log_name, iter_num, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m):\n\u001b[0;32m--> 228\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mTRAINED_MODEL_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_timesteps\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/stable_baselines3/ddpg/ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[1;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/stable_baselines3/td3/td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[1;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
            "File \u001b[0;32m~/trading/modules/FinRL/finrl/meta/env_stock_trading/env_stocktrading.py:221\u001b[0m, in \u001b[0;36mStockTradingEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal:\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;66;03m# print(f\"Episode: {self.episode}\")\u001b[39;00m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_plots:\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/pandas/core/indexes/base.py:3068\u001b[0m, in \u001b[0;36mIndex.unique\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m   3066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_view()\n\u001b[0;32m-> 3068\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shallow_copy(result)\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/finrl/lib/python3.11/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
        "                                                 PPO_model_kwargs,\n",
        "                                                 DDPG_model_kwargs,\n",
        "                                                 SAC_model_kwargs,\n",
        "                                                 TD3_model_kwargs,\n",
        "                                                 timesteps_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "-0qd8acMtj1f",
        "outputId": "ef19ff5d-9173-4268-e50b-6128cf9278f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iter</th>\n",
              "      <th>Val Start</th>\n",
              "      <th>Val End</th>\n",
              "      <th>Model Used</th>\n",
              "      <th>A2C Sharpe</th>\n",
              "      <th>PPO Sharpe</th>\n",
              "      <th>DDPG Sharpe</th>\n",
              "      <th>SAC Sharpe</th>\n",
              "      <th>TD3 Sharpe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Iter, Val Start, Val End, Model Used, A2C Sharpe, PPO Sharpe, DDPG Sharpe, SAC Sharpe, TD3 Sharpe]\n",
              "Index: []"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4JKB--8tj1g"
      },
      "outputs": [],
      "source": [
        "unique_trade_date = processed[(processed.date > val_test_period[0])&(processed.date <= val_test_period[1])].date.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9mKF7GGtj1g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
        "\n",
        "df_account_value=pd.DataFrame()\n",
        "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
        "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
        "    df_account_value = df_account_value.append(temp,ignore_index=True)\n",
        "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
        "print('Sharpe Ratio: ',sharpe)\n",
        "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyosyW7_tj1g"
      },
      "outputs": [],
      "source": [
        "df_account_value.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLsRdw2Ctj1h"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df_account_value.account_value.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr2zX7ZxNyFQ"
      },
      "source": [
        "<a id='6.1'></a>\n",
        "## 7.1 BackTestStats\n",
        "pass in df_account_value, this information is stored in env class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzkr9yv-AdV_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"==============Get Backtest Results===========\")\n",
        "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
        "perf_stats_all = pd.DataFrame(perf_stats_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiHhM1YkoCel"
      },
      "outputs": [],
      "source": [
        "#baseline stats\n",
        "print(\"==============Get Baseline Stats===========\")\n",
        "df_dji_ = get_baseline(\n",
        "        ticker=\"^DJI\",\n",
        "        start = df_account_value.loc[0,'date'],\n",
        "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
        "\n",
        "stats = backtest_stats(df_dji_, value_col_name = 'close')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhJ9whD75WTs"
      },
      "outputs": [],
      "source": [
        "df_dji = pd.DataFrame()\n",
        "df_dji['date'] = df_account_value['date']\n",
        "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
        "print(\"df_dji: \", df_dji)\n",
        "df_dji.to_csv(\"df_dji.csv\")\n",
        "df_dji = df_dji.set_index(df_dji.columns[0])\n",
        "print(\"df_dji: \", df_dji)\n",
        "df_dji.to_csv(\"df_dji+.csv\")\n",
        "\n",
        "df_account_value.to_csv('df_account_value.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U6Suru3h1jc"
      },
      "source": [
        "<a id='6.2'></a>\n",
        "## 7.2 BackTestPlot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HggausPRoCem"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# print(\"==============Compare to DJIA===========\")\n",
        "# %matplotlib inline\n",
        "# # S&P 500: ^GSPC\n",
        "# # Dow Jones Index: ^DJI\n",
        "# # NASDAQ 100: ^NDX\n",
        "# backtest_plot(df_account_value,\n",
        "#               baseline_ticker = '^DJI',\n",
        "#               baseline_start = df_account_value.loc[0,'date'],\n",
        "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
        "df.to_csv(\"df.csv\")\n",
        "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
        "df_result_ensemble = df_result_ensemble.set_index('date')\n",
        "\n",
        "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
        "\n",
        "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
        "print(\"df_trade_date: \", df_trade_date)\n",
        "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
        "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
        "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
        "print(\"df_result_ensemble: \", df_result_ensemble)\n",
        "print(\"==============Compare to DJIA===========\")\n",
        "result = pd.DataFrame()\n",
        "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
        "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
        "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
        "print(\"result: \", result)\n",
        "result.to_csv(\"result.csv\")\n",
        "result.columns = ['ensemble', 'dji']\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "plt.figure();\n",
        "result.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBQx4bVQFi-a"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
